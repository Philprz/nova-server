# services/field_analyzer.py
"""
Analyseur de champs pour identifier les patterns et champs obligatoires
dans Salesforce et SAP pour la cr√©ation de clients
"""

import os
import sys
import json
import asyncio
import logging
from datetime import datetime
from typing import Dict, List, Any
from collections import Counter
from mcp_connector import MCPConnector
# Ajouter le r√©pertoire racine au path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Configuration des logs
os.makedirs('logs', exist_ok=True)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'logs/field_analyzer_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('field_analyzer')



class FieldAnalyzer:
    """Analyseur de champs pour Salesforce et SAP"""
    
    def __init__(self):
        self.analysis_results = {
            "salesforce": {
                "metadata": None,
                "sample_analysis": None,
                "field_patterns": None
            },
            "sap": {
                "metadata": None,
                "sample_analysis": None,
                "field_patterns": None
            },
            "mapping_suggestions": None,
            "creation_requirements": None,
            "timestamp": datetime.now().isoformat()
        }
    
    async def analyze_all(self, sample_size: int = 50) -> Dict[str, Any]:
        """
        Lance l'analyse compl√®te des deux syst√®mes
        
        Args:
            sample_size: Nombre d'enregistrements √† analyser par syst√®me
        """
        logger.info("=== D√âBUT DE L'ANALYSE DES CHAMPS ===")
        logger.info(f"Taille d'√©chantillon: {sample_size} enregistrements par syst√®me")
        
        try:
            # Analyse Salesforce
            logger.info("--- Analyse Salesforce ---")
            sf_analysis = await self._analyze_salesforce(sample_size)
            self.analysis_results["salesforce"] = sf_analysis
            
            # Analyse SAP
            logger.info("--- Analyse SAP ---")
            sap_analysis = await self._analyze_sap(sample_size)
            self.analysis_results["sap"] = sap_analysis
            
            # G√©n√©ration des suggestions de mapping
            logger.info("--- G√©n√©ration des suggestions de mapping ---")
            mapping_suggestions = self._generate_mapping_suggestions()
            self.analysis_results["mapping_suggestions"] = mapping_suggestions
            
            # G√©n√©ration des exigences de cr√©ation
            logger.info("--- G√©n√©ration des exigences de cr√©ation ---")
            creation_requirements = self._generate_creation_requirements()
            self.analysis_results["creation_requirements"] = creation_requirements
            
            # Sauvegarde des r√©sultats
            await self._save_analysis_results()
            
            # G√©n√©ration du rapport
            self._generate_summary_report()
            
            logger.info("=== ANALYSE TERMIN√âE ===")
            return self.analysis_results
            
        except Exception as e:
            logger.exception(f"Erreur lors de l'analyse: {str(e)}")
            return {"error": str(e)}
    
    async def _analyze_salesforce(self, sample_size: int) -> Dict[str, Any]:
        """Analyse les comptes Salesforce"""
        logger.info("Analyse des comptes Salesforce...")
        
        try:
            # 1. R√©cup√©rer les m√©tadonn√©es des comptes
            logger.info("R√©cup√©ration des m√©tadonn√©es Account...")
            metadata_result = await MCPConnector.call_salesforce_mcp("salesforce_inspect", {
                "object_name": "Account"
            })
            
            if "error" in metadata_result:
                logger.error(f"Erreur m√©tadonn√©es Salesforce: {metadata_result['error']}")
                return {"error": metadata_result["error"]}
            
            # 2. R√©cup√©rer un √©chantillon de comptes
            logger.info(f"R√©cup√©ration d'un √©chantillon de {sample_size} comptes...")
            
            # Construire la requ√™te avec tous les champs importants
            important_fields = [
                "Id", "Name", "AccountNumber", "Type", "Industry", "AnnualRevenue", 
                "NumberOfEmployees", "Phone", "Fax", "Website", "Description",
                "BillingStreet", "BillingCity", "BillingState", "BillingPostalCode", "BillingCountry",
                "ShippingStreet", "ShippingCity", "ShippingState", "ShippingPostalCode", "ShippingCountry",
                "ParentId", "OwnerId", "Rating", "SicDesc", "TickerSymbol", "AccountSource",
                "CreatedDate", "LastModifiedDate"
            ]
            
            query = f"SELECT {', '.join(important_fields)} FROM Account ORDER BY CreatedDate DESC LIMIT {sample_size}"
            
            sample_result = await MCPConnector.call_salesforce_mcp("salesforce_query", {
                "query": query
            })
            
            if "error" in sample_result:
                logger.error(f"Erreur √©chantillon Salesforce: {sample_result['error']}")
                return {"error": sample_result["error"]}
            
            # 3. Analyser l'√©chantillon
            sample_accounts = sample_result.get("records", [])
            logger.info(f"Analyse de {len(sample_accounts)} comptes Salesforce...")
            
            field_analysis = self._analyze_field_patterns(sample_accounts, "Salesforce")
            
            return {
                "metadata": metadata_result,
                "sample_size": len(sample_accounts),
                "sample_analysis": field_analysis,
                "field_patterns": self._extract_salesforce_patterns(sample_accounts)
            }
            
        except Exception as e:
            logger.exception(f"Erreur analyse Salesforce: {str(e)}")
            return {"error": str(e)}
    
    async def _analyze_sap(self, sample_size: int) -> Dict[str, Any]:
        """Analyse les clients SAP (Business Partners)"""
        logger.info("Analyse des clients SAP...")
        
        try:
            # 1. R√©cup√©rer un √©chantillon de clients SAP
            logger.info(f"R√©cup√©ration d'un √©chantillon de {sample_size} clients SAP...")
            
            # Requ√™te pour r√©cup√©rer les Business Partners de type Customer
            endpoint = f"/BusinessPartners?$filter=CardType eq 'cCustomer'&$orderby=CreateDate desc&$top={sample_size}"
            
            sample_result = await MCPConnector.call_sap_mcp("sap_read", {
                "endpoint": endpoint,
                "method": "GET"
            })
            
            if "error" in sample_result:
                logger.error(f"Erreur √©chantillon SAP: {sample_result['error']}")
                return {"error": sample_result["error"]}
            
            # 2. Analyser l'√©chantillon
            sample_customers = sample_result.get("value", [])
            logger.info(f"Analyse de {len(sample_customers)} clients SAP...")
            
            if not sample_customers:
                logger.warning("Aucun client SAP trouv√© dans l'√©chantillon")
                return {"error": "Aucun client SAP trouv√©"}
            
            field_analysis = self._analyze_field_patterns(sample_customers, "SAP")
            
            # 3. Analyser la structure des champs SAP
            sap_patterns = self._extract_sap_patterns(sample_customers)
            
            return {
                "sample_size": len(sample_customers),
                "sample_analysis": field_analysis,
                "field_patterns": sap_patterns,
                "metadata": {
                    "analyzed_fields": list(sample_customers[0].keys()) if sample_customers else [],
                    "total_fields": len(sample_customers[0].keys()) if sample_customers else 0
                }
            }
            
        except Exception as e:
            logger.exception(f"Erreur analyse SAP: {str(e)}")
            return {"error": str(e)}
    
    def _analyze_field_patterns(self, records: List[Dict], system_name: str) -> Dict[str, Any]:
        """Analyse les patterns dans les champs d'un √©chantillon d'enregistrements"""
        if not records:
            return {"error": "Aucun enregistrement √† analyser"}
        
        total_records = len(records)
        field_stats = {}
        
        # Analyser chaque champ
        for field_name in records[0].keys():
            values = []
            non_null_count = 0
            empty_string_count = 0
            unique_values = set()
            
            for record in records:
                value = record.get(field_name)
                values.append(value)
                
                if value is not None:
                    non_null_count += 1
                    if isinstance(value, str):
                        if value.strip() == "":
                            empty_string_count += 1
                        else:
                            unique_values.add(value)
                    else:
                        unique_values.add(str(value))
            
            # Calculer les statistiques
            fill_rate = (non_null_count / total_records) * 100
            non_empty_rate = ((non_null_count - empty_string_count) / total_records) * 100 if non_null_count > 0 else 0
            uniqueness_rate = (len(unique_values) / non_null_count) * 100 if non_null_count > 0 else 0
            
            field_stats[field_name] = {
                "fill_rate": fill_rate,
                "non_empty_rate": non_empty_rate,
                "uniqueness_rate": uniqueness_rate,
                "total_records": total_records,
                "non_null_count": non_null_count,
                "unique_values_count": len(unique_values),
                "sample_values": list(unique_values)[:5],  # 5 premiers exemples
                "is_likely_required": fill_rate >= 95.0,
                "is_business_critical": non_empty_rate >= 80.0
            }
        
        # Identifier les champs probablement obligatoires
        required_fields = [field for field, stats in field_stats.items() if stats["is_likely_required"]]
        business_critical_fields = [field for field, stats in field_stats.items() if stats["is_business_critical"]]
        
        logger.info(f"Analyse {system_name} - {total_records} enregistrements:")
        logger.info(f"  - Champs probablement obligatoires: {len(required_fields)}")
        logger.info(f"  - Champs critiques m√©tier: {len(business_critical_fields)}")
        
        return {
            "total_records": total_records,
            "field_statistics": field_stats,
            "required_fields": required_fields,
            "business_critical_fields": business_critical_fields,
            "total_fields_analyzed": len(field_stats)
        }
    
    def _extract_salesforce_patterns(self, accounts: List[Dict]) -> Dict[str, Any]:
        """Extrait les patterns sp√©cifiques √† Salesforce"""
        patterns = {
            "address_patterns": {
                "billing_complete": 0,
                "shipping_complete": 0,
                "both_addresses": 0
            },
            "industry_distribution": Counter(),
            "type_distribution": Counter(),
            "common_field_combinations": [],
            "data_quality_indicators": {}
        }
        
        for account in accounts:
            # Analyse des adresses
            billing_complete = all([
                account.get("BillingStreet"), 
                account.get("BillingCity"), 
                account.get("BillingCountry")
            ])
            shipping_complete = all([
                account.get("ShippingStreet"), 
                account.get("ShippingCity"), 
                account.get("ShippingCountry")
            ])
            
            if billing_complete:
                patterns["address_patterns"]["billing_complete"] += 1
            if shipping_complete:
                patterns["address_patterns"]["shipping_complete"] += 1
            if billing_complete and shipping_complete:
                patterns["address_patterns"]["both_addresses"] += 1
            
            # Distribution des industries et types
            if account.get("Industry"):
                patterns["industry_distribution"][account["Industry"]] += 1
            if account.get("Type"):
                patterns["type_distribution"][account["Type"]] += 1
        
        return patterns
    
    def _extract_sap_patterns(self, customers: List[Dict]) -> Dict[str, Any]:
        """Extrait les patterns sp√©cifiques √† SAP"""
        patterns = {
            "card_type_distribution": Counter(),
            "group_code_distribution": Counter(),
            "currency_distribution": Counter(),
            "address_patterns": {
                "bill_to_complete": 0,
                "ship_to_complete": 0,
                "both_addresses": 0
            },
            "common_prefixes": Counter(),
            "data_quality_indicators": {}
        }
        
        for customer in customers:
            # Distribution des types de carte
            if customer.get("CardType"):
                patterns["card_type_distribution"][customer["CardType"]] += 1
            
            # Distribution des groupes
            if customer.get("GroupCode"):
                patterns["group_code_distribution"][customer["GroupCode"]] += 1
            
            # Distribution des devises
            if customer.get("Currency"):
                patterns["currency_distribution"][customer["Currency"]] += 1
            
            # Analyse des adresses SAP
            bill_to_complete = all([
                customer.get("BillToStreet"), 
                customer.get("BillToCity"), 
                customer.get("BillToCountry")
            ])
            ship_to_complete = all([
                customer.get("ShipToStreet"), 
                customer.get("ShipToCity"), 
                customer.get("ShipToCountry")
            ])
            
            if bill_to_complete:
                patterns["address_patterns"]["bill_to_complete"] += 1
            if ship_to_complete:
                patterns["address_patterns"]["ship_to_complete"] += 1
            if bill_to_complete and ship_to_complete:
                patterns["address_patterns"]["both_addresses"] += 1
            
            # Analyse des pr√©fixes de CardCode
            card_code = customer.get("CardCode", "")
            if len(card_code) >= 2:
                prefix = card_code[:2]
                patterns["common_prefixes"][prefix] += 1
        
        return patterns
    
    def _generate_mapping_suggestions(self) -> Dict[str, Any]:
        """G√©n√®re des suggestions de mapping entre Salesforce et SAP"""
        logger.info("G√©n√©ration des suggestions de mapping...")
        
        # Mapping bas√© sur l'analyse des noms de champs et des patterns
        field_mappings = {
            # Informations de base
            "Name": "CardName",
            "AccountNumber": "CardCode",
            "Phone": "Phone1",
            "Fax": "Fax",
            "Website": "Website",
            
            # Adresse de facturation
            "BillingStreet": "BillToStreet",
            "BillingCity": "BillToCity",
            "BillingState": "BillToState",
            "BillingPostalCode": "BillToZipCode",
            "BillingCountry": "BillToCountry",
            
            # Adresse de livraison
            "ShippingStreet": "ShipToStreet",
            "ShippingCity": "ShipToCity",
            "ShippingState": "ShipToState",
            "ShippingPostalCode": "ShipToZipCode",
            "ShippingCountry": "ShipToCountry",
            
            # Informations m√©tier
            "Industry": "Industry",
            "Description": "Notes",
            "AnnualRevenue": "DunningTerm",  # Approximation
            
            # Champs calcul√©s/constants pour SAP
            "CONSTANTS": {
                "CardType": "cCustomer",
                "GroupCode": 100,  # Groupe par d√©faut
                "Currency": "EUR",
                "Valid": "tYES",
                "Frozen": "tNO"
            }
        }
        
        # G√©n√©ration des r√®gles de transformation
        transformation_rules = {
            "CardCode": {
                "source": "Name",
                "transformation": "generate_from_name",
                "description": "G√©n√©rer un CardCode unique bas√© sur le nom"
            },
            "GroupCode": {
                "source": "Type",
                "transformation": "map_type_to_group",
                "description": "Mapper le type Salesforce vers un groupe SAP"
            }
        }
        
        return {
            "field_mappings": field_mappings,
            "transformation_rules": transformation_rules,
            "confidence_level": "high",
            "validation_needed": [
                "V√©rifier les codes de groupe SAP disponibles",
                "Valider les codes de devise accept√©s",
                "Confirmer les formats d'adresse"
            ]
        }
    
    def _generate_creation_requirements(self) -> Dict[str, Any]:
        """G√©n√®re les exigences pour la cr√©ation de clients"""
        logger.info("G√©n√©ration des exigences de cr√©ation...")
        
        sf_analysis = self.analysis_results.get("salesforce", {}).get("sample_analysis", {})
        sap_analysis = self.analysis_results.get("sap", {}).get("sample_analysis", {})
        
        requirements = {
            "salesforce": {
                "absolutely_required": [],
                "business_required": [],
                "optional": []
            },
            "sap": {
                "absolutely_required": [],
                "business_required": [],
                "optional": []
            },
            "workflow_requirements": {},
            "validation_rules": {}
        }
        
        # Analyser les exigences Salesforce
        if sf_analysis and "field_statistics" in sf_analysis:
            for field, stats in sf_analysis["field_statistics"].items():
                if stats["is_likely_required"]:
                    requirements["salesforce"]["absolutely_required"].append(field)
                elif stats["is_business_critical"]:
                    requirements["salesforce"]["business_required"].append(field)
                else:
                    requirements["salesforce"]["optional"].append(field)
        
        # Analyser les exigences SAP
        if sap_analysis and "field_statistics" in sap_analysis:
            for field, stats in sap_analysis["field_statistics"].items():
                if stats["is_likely_required"]:
                    requirements["sap"]["absolutely_required"].append(field)
                elif stats["is_business_critical"]:
                    requirements["sap"]["business_required"].append(field)
                else:
                    requirements["sap"]["optional"].append(field)
        
        # D√©finir les exigences du workflow
        requirements["workflow_requirements"] = {
            "minimum_required_from_user": [
                "company_name",
                "contact_info",  # phone ou email
                "primary_address"
            ],
            "auto_generated": [
                "account_number",
                "creation_date",
                "default_settings"
            ],
            "optional_from_user": [
                "industry",
                "annual_revenue",
                "description",
                "shipping_address"
            ]
        }
        
        # R√®gles de validation
        requirements["validation_rules"] = {
            "company_name": {
                "min_length": 2,
                "max_length": 100,
                "pattern": "^[a-zA-Z0-9\\s\\-\\.&]+$"
            },
            "phone": {
                "pattern": "^[+]?[0-9\\s\\-\\.\\(\\)]+$",
                "min_length": 8
            },
            "email": {
                "pattern": "^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$"
            },
            "postal_code": {
                "pattern": "^[0-9]{5}$|^[0-9]{5}-[0-9]{4}$"
            }
        }
        
        return requirements
    
    async def _save_analysis_results(self):
        """Sauvegarde les r√©sultats d'analyse"""
        try:
            filename = f"field_analysis_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(self.analysis_results, f, indent=2, ensure_ascii=False, default=str)
            
            logger.info(f"üìÅ R√©sultats d'analyse sauvegard√©s: {filename}")
            
        except Exception as e:
            logger.warning(f"Impossible de sauvegarder les r√©sultats: {str(e)}")
    
    def _generate_summary_report(self):
        """G√©n√®re un rapport de synth√®se lisible"""
        logger.info("\n" + "="*80)
        logger.info("üìä RAPPORT DE SYNTH√àSE - ANALYSE DES CHAMPS")
        logger.info("="*80)
        
        # R√©sum√© Salesforce
        sf_analysis = self.analysis_results.get("salesforce", {}).get("sample_analysis", {})
        if sf_analysis:
            logger.info(f"\nüî∑ SALESFORCE - {sf_analysis.get('total_records', 0)} comptes analys√©s")
            logger.info(f"   Champs obligatoires identifi√©s: {len(sf_analysis.get('required_fields', []))}")
            logger.info(f"   Champs critiques m√©tier: {len(sf_analysis.get('business_critical_fields', []))}")
            
            if sf_analysis.get('required_fields'):
                logger.info(f"   ‚Üí Obligatoires: {', '.join(sf_analysis['required_fields'][:5])}...")
        
        # R√©sum√© SAP
        sap_analysis = self.analysis_results.get("sap", {}).get("sample_analysis", {})
        if sap_analysis:
            logger.info(f"\nüî∂ SAP - {sap_analysis.get('total_records', 0)} clients analys√©s")
            logger.info(f"   Champs obligatoires identifi√©s: {len(sap_analysis.get('required_fields', []))}")
            logger.info(f"   Champs critiques m√©tier: {len(sap_analysis.get('business_critical_fields', []))}")
            
            if sap_analysis.get('required_fields'):
                logger.info(f"   ‚Üí Obligatoires: {', '.join(sap_analysis['required_fields'][:5])}...")
        
        # R√©sum√© du mapping
        mapping = self.analysis_results.get("mapping_suggestions", {})
        if mapping:
            logger.info("\nüîÑ MAPPING IDENTIFI√â")
            logger.info(f"   Correspondances directes: {len(mapping.get('field_mappings', {}))}")
            logger.info(f"   Transformations n√©cessaires: {len(mapping.get('transformation_rules', {}))}")
        
        # Recommandations
        requirements = self.analysis_results.get("creation_requirements", {})
        if requirements:
            logger.info("\n‚úÖ RECOMMANDATIONS")
            workflow_req = requirements.get("workflow_requirements", {})
            if workflow_req:
                min_required = workflow_req.get("minimum_required_from_user", [])
                logger.info(f"   Minimum requis de l'utilisateur: {', '.join(min_required)}")
        
        logger.info("\n" + "="*80)
        logger.info("üìã PROCHAINES √âTAPES RECOMMAND√âES:")
        logger.info("   1. Valider les mappings identifi√©s")
        logger.info("   2. Impl√©menter le workflow de cr√©ation client")
        logger.info("   3. Cr√©er les r√®gles de validation")
        logger.info("   4. Tester avec des donn√©es r√©elles")
        logger.info("="*80)

# Fonctions utilitaires pour usage direct
async def analyze_customer_fields(sample_size: int = 50):
    """Lance l'analyse compl√®te des champs clients"""
    analyzer = FieldAnalyzer()
    return await analyzer.analyze_all(sample_size)

async def quick_analysis(sample_size: int = 10):
    """Lance une analyse rapide avec un petit √©chantillon"""
    logger.info("üöÄ Analyse rapide des champs clients")
    return await analyze_customer_fields(sample_size)

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Analyse des champs clients Salesforce et SAP")
    parser.add_argument("--sample-size", type=int, default=50, help="Taille de l'√©chantillon √† analyser")
    parser.add_argument("--quick", action="store_true", help="Analyse rapide avec 10 enregistrements")
    
    args = parser.parse_args()
    
    if args.quick:
        logger.info("üöÄ Mode analyse rapide")
        results = asyncio.run(quick_analysis())
    else:
        logger.info(f"üöÄ Analyse compl√®te avec √©chantillon de {args.sample_size}")
        results = asyncio.run(analyze_customer_fields(args.sample_size))
    
    if results and "error" not in results:
        print("\n‚úÖ Analyse termin√©e avec succ√®s !")
        print("üìÅ V√©rifiez les fichiers g√©n√©r√©s pour les d√©tails complets")
    else:
        print(f"‚ùå Erreur lors de l'analyse: {results.get('error', 'Erreur inconnue')}")